{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "JKtnQ7Y1_J30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JKtnQ7Y1_J30",
        "outputId": "736e8065-d53b-4c67-9f8c-ddbee0bb0840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.3.1\n",
            "  Downloading torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchtext==0.18.0\n",
            "  Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.1) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "Downloading torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchtext\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.3.1 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 portalocker-3.2.0 torch-2.3.1 torchtext-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.3.1 torchtext==0.18.0 portalocker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "AdByqGW3ZAXZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AdByqGW3ZAXZ",
        "outputId": "2f556b6c-79f4-415f-8177-5af1ea1ff826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'NLP'...\n",
            "remote: Enumerating objects: 159, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 159 (delta 43), reused 46 (delta 18), pack-reused 72 (from 1)\u001b[K\n",
            "Receiving objects: 100% (159/159), 51.07 MiB | 41.44 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!git clone  https://github.com/NgThanhNhanf/NLP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "739bf4d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "739bf4d4",
        "outputId": "79566b82-0642-4089-edee-f07a3b9962ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/12.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.8/12.8 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.6/12.8 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m197.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m197.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b59a600a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b59a600a",
        "outputId": "0f9b43d1-bb86-4ceb-d6d8-289b7b78a89d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e05afd97",
      "metadata": {
        "id": "e05afd97"
      },
      "source": [
        "After downloading, you can re-run the initialization cell (`b2bf295b`) to confirm that Spacy loads the models correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "mJo_QEZ3eFc-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJo_QEZ3eFc-",
        "outputId": "11a3f710-1fd5-4c13-dbc3-bbcce67a3e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NLP\n"
          ]
        }
      ],
      "source": [
        "%cd /content/NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "beKp8unreKcq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "beKp8unreKcq",
        "outputId": "9619deac-6739-45e8-d51c-9ad67dfa0226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.3.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.18.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (3.8.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (3.9.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (6.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (0.20.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (2.12.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy->-r requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 4)) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 4)) (2025.11.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 5)) (2.9.0.post0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r requirements.txt (line 8)) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r requirements.txt (line 8)) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r requirements.txt (line 8)) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r requirements.txt (line 8)) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r requirements.txt (line 8)) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r requirements.txt (line 8)) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r requirements.txt (line 8)) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->-r requirements.txt (line 8)) (5.7.1)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 8)) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 8)) (5.9.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext->-r requirements.txt (line 2)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext->-r requirements.txt (line 2)) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 3)) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 3)) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy->-r requirements.txt (line 3)) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy->-r requirements.txt (line 3)) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.8.5)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 8)) (4.5.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->-r requirements.txt (line 8)) (0.2.14)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy->-r requirements.txt (line 3)) (2.0.1)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "37P1IFku9Uew",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37P1IFku9Uew",
        "outputId": "2894eedb-bbf6-4a04-9bf2-8c02f7ece0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thi·∫øt b·ªã ƒëang s·ª≠ d·ª•ng: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from src.model_base import Encoder, Decoder, Seq2Seq\n",
        "from src.dataset import get_data_loaders, build_vocab_and_tokenizers\n",
        "\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Ki·ªÉm tra GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Thi·∫øt b·ªã ƒëang s·ª≠ d·ª•ng: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "PIP0WfvnNYrC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIP0WfvnNYrC",
        "outputId": "2ca1a5ee-623e-44d2-de14-7952533b4a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ƒêang x·ª≠ l√Ω d·ªØ li·ªáu...\n",
            "ƒêang x√¢y d·ª±ng t·ª´ ƒëi·ªÉn (Vocabulary)...\n",
            "ƒêang t·∫°o Dataset...\n",
            "ƒêang t·∫°o DataLoader...\n",
            "S·ªë l∆∞·ª£ng c√¢u Train: 29024\n",
            "K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn Anh: 6191\n",
            "K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn Ph√°p: 6555\n"
          ]
        }
      ],
      "source": [
        "print(\"ƒêang x·ª≠ l√Ω d·ªØ li·ªáu...\")\n",
        "\n",
        "# 1. ƒê∆∞·ªùng d·∫´n file (ƒê·∫£m b·∫£o b·∫°n ƒë√£ ƒë·ªïi t√™n file th√†nh test.en, test.fr nh∆∞ m√¨nh d·∫∑n)\n",
        "SRC_FILE = 'data/raw/train.en'\n",
        "TRG_FILE = 'data/raw/train.fr' # Ho·∫∑c t∆∞∆°ng t·ª± cho Val/Test\n",
        "\n",
        "# 2. X√¢y d·ª±ng Vocab & Tokenizer (G·ªçi h√†m t·ª´ dataset.py c·ªßa b·∫°n)\n",
        "# L∆∞u √Ω: B·∫°n c·∫ßn ch·ªânh s·ª≠a d√≤ng n√†y cho kh·ªõp v·ªõi t√™n h√†m b·∫°n vi·∫øt trong src/dataset.py\n",
        "# M·ª•c ti√™u l√† l·∫•y ƒë∆∞·ª£c object vocab ƒë·ªÉ bi·∫øt k√≠ch th∆∞·ªõc input/output\n",
        "\n",
        "# 3. T·∫°o Iterators (DataLoaders)\n",
        "BATCH_SIZE = 32 # ƒê·ªÅ b√†i g·ª£i √Ω 32-128\n",
        "src_vocab, trg_vocab, train_iterator, valid_iterator, test_iterator, src_tokenizer, trg_tokenizer = get_data_loaders(batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"S·ªë l∆∞·ª£ng c√¢u Train: {len(train_iterator) * BATCH_SIZE}\")\n",
        "print(f\"K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn Anh: {len(src_vocab)}\")\n",
        "print(f\"K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn Ph√°p: {len(trg_vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "v9_gXicKOdIi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9_gXicKOdIi",
        "outputId": "3b4bc163-c662-4fc7-a1ad-eaeb4f4d3b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M√¥ h√¨nh c√≥ 19,342,235 tham s·ªë c√≥ th·ªÉ hu·∫•n luy·ªán\n"
          ]
        }
      ],
      "source": [
        "# --- HYPERPARAMETERS ---\n",
        "INPUT_DIM = len(src_vocab)\n",
        "OUTPUT_DIM = len(trg_vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "# --- KH·ªûI T·∫†O ---\n",
        "enc = Encoder(INPUT_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# --- KH·ªûI T·∫†O TR·ªåNG S·ªê (WEIGHTS) ---\n",
        "# Gi√∫p model h·ªçc nhanh h∆°n thay v√¨ ƒë·ªÉ random m·∫∑c ƒë·ªãnh\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "# ƒê·∫øm s·ªë l∆∞·ª£ng tham s·ªë (ƒê·ªÉ ghi v√†o b√°o c√°o cho o√°ch)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'M√¥ h√¨nh c√≥ {count_parameters(model):,} tham s·ªë c√≥ th·ªÉ hu·∫•n luy·ªán')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8bkmb9HQOjGT",
      "metadata": {
        "id": "8bkmb9HQOjGT"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# B·ªè qua padding token khi t√≠nh loss\n",
        "TRG_PAD_IDX = trg_vocab['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3lg93jLsOm_L",
      "metadata": {
        "id": "3lg93jLsOm_L"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, criterion, optimizer,\n",
        "                        num_epochs, device, clip=1.0, teacher_forcing_ratio=0.5,\n",
        "                        patience=3, model_path='best_model.pth'):\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"üöÄ B·∫Øt ƒë·∫ßu training v·ªõi {num_epochs} epochs\")\n",
        "    print(f\"   Teacher forcing ratio: {teacher_forcing_ratio}\")\n",
        "    print(f\"   Gradient clip: {clip}\")\n",
        "    print(f\"   Early stopping patience: {patience}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # === TRAINING PHASE ===\n",
        "        model.train()\n",
        "        epoch_train_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        # Th√™m progress bar ƒë∆°n gi·∫£n\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs} - Training...\")\n",
        "\n",
        "        for batch_idx, (source, target, source_lengths) in enumerate(train_loader):\n",
        "            source, target = source.to(device), target.to(device)\n",
        "            source_lengths = source_lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward v·ªõi teacher forcing\n",
        "            output = model(source, target, source_lengths, teacher_forcing_ratio)\n",
        "\n",
        "            # T√≠nh loss (b·ªè <sos> token)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            target = target[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping v·ªõi tham s·ªë\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            # In progress m·ªói 50 batch\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"   Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = epoch_train_loss / batch_count\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # === VALIDATION PHASE ===\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Validating...\")\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0\n",
        "        batch_count_val = 0\n",
        "\n",
        "        # L·∫•y m·ªôt batch t·ª´ DataLoader\n",
        "        batch = next(iter(val_loader))\n",
        "\n",
        "        print(f\"S·ªë ph·∫ßn t·ª≠ trong batch: {len(batch)}\")\n",
        "        print(f\"Ki·ªÉu d·ªØ li·ªáu batch: {type(batch)}\")\n",
        "\n",
        "        # Ki·ªÉm tra t·ª´ng ph·∫ßn t·ª≠\n",
        "        for i, item in enumerate(batch):\n",
        "            print(f\"  Ph·∫ßn t·ª≠ {i}: type={type(item)}, shape={item.shape if hasattr(item, 'shape') else len(item)}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for source, target, source_lengths in val_loader:\n",
        "                source, target = source.to(device), target.to(device)\n",
        "\n",
        "                # Evaluation: teacher forcing = 0\n",
        "                output = model(source, target, source_lengths, teacher_forcing_ratio=0)\n",
        "\n",
        "                output_dim = output.shape[-1]\n",
        "                output = output[:, 1:].reshape(-1, output_dim)\n",
        "                target = target[:, 1:].reshape(-1)\n",
        "\n",
        "                loss = criterion(output, target)\n",
        "                epoch_val_loss += loss.item()\n",
        "                batch_count_val += 1\n",
        "\n",
        "        avg_val_loss = epoch_val_loss / batch_count_val\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # === IN K·∫æT QU·∫¢ ===\n",
        "        print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
        "        print(f\"   Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"   Val Loss:   {avg_val_loss:.4f}\")\n",
        "        print(f\"   Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # === EARLY STOPPING & SAVE BEST MODEL ===\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': avg_train_loss,\n",
        "                'val_loss': avg_val_loss,\n",
        "            }, model_path)\n",
        "            print(f\"   üíæ Best model saved! (Loss: {avg_val_loss:.4f})\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"   ‚è≥ No improvement ({patience_counter}/{patience})\")\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\nüõë Early stopping triggered after {epoch+1} epochs!\")\n",
        "                print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
        "                break\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    print(f\"\\nüéâ Training completed!\")\n",
        "    print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
        "    print(f\"   Total epochs trained: {len(train_losses)}\")\n",
        "\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "M8WLff0_OoI7",
      "metadata": {
        "id": "M8WLff0_OoI7",
        "outputId": "3464d568-f8d2-4056-b046-11f1b8ec93de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ B·∫Øt ƒë·∫ßu training v·ªõi 15 epochs\n",
            "   Teacher forcing ratio: 0.5\n",
            "   Gradient clip: 1\n",
            "   Early stopping patience: 3\n",
            "============================================================\n",
            "\n",
            "Epoch 1/15 - Training...\n",
            "   Batch 0/907, Loss: 8.7877\n",
            "   Batch 50/907, Loss: 5.0734\n",
            "   Batch 100/907, Loss: 4.9705\n",
            "   Batch 150/907, Loss: 5.1684\n",
            "   Batch 200/907, Loss: 4.9625\n",
            "   Batch 250/907, Loss: 4.7369\n",
            "   Batch 300/907, Loss: 4.6929\n",
            "   Batch 350/907, Loss: 4.3177\n",
            "   Batch 400/907, Loss: 4.5011\n",
            "   Batch 450/907, Loss: 4.5471\n",
            "   Batch 500/907, Loss: 4.1392\n",
            "   Batch 550/907, Loss: 4.1061\n",
            "   Batch 600/907, Loss: 4.2407\n",
            "   Batch 650/907, Loss: 3.9184\n",
            "   Batch 700/907, Loss: 3.9217\n",
            "   Batch 750/907, Loss: 3.9439\n",
            "   Batch 800/907, Loss: 3.5715\n",
            "   Batch 850/907, Loss: 4.3937\n",
            "   Batch 900/907, Loss: 3.7072\n",
            "Epoch 1/15 - Validating...\n",
            "S·ªë ph·∫ßn t·ª≠ trong batch: 3\n",
            "Ki·ªÉu d·ªØ li·ªáu batch: <class 'tuple'>\n",
            "  Ph·∫ßn t·ª≠ 0: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 1: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 2: type=<class 'torch.Tensor'>, shape=torch.Size([32])\n",
            "\n",
            "üìä Epoch 1 Summary:\n",
            "   Train Loss: 4.5020\n",
            "   Val Loss:   4.5217\n",
            "   Learning Rate: 0.001000\n",
            "   üíæ Best model saved! (Loss: 4.5217)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 2/15 - Training...\n",
            "   Batch 0/907, Loss: 3.7831\n",
            "   Batch 50/907, Loss: 3.5639\n",
            "   Batch 100/907, Loss: 3.7282\n",
            "   Batch 150/907, Loss: 3.4852\n",
            "   Batch 200/907, Loss: 3.5836\n",
            "   Batch 250/907, Loss: 3.5218\n",
            "   Batch 300/907, Loss: 3.5643\n",
            "   Batch 350/907, Loss: 3.7706\n",
            "   Batch 400/907, Loss: 3.6323\n",
            "   Batch 450/907, Loss: 3.3869\n",
            "   Batch 500/907, Loss: 3.9957\n",
            "   Batch 550/907, Loss: 3.7994\n",
            "   Batch 600/907, Loss: 3.3577\n",
            "   Batch 650/907, Loss: 3.5024\n",
            "   Batch 700/907, Loss: 3.3891\n",
            "   Batch 750/907, Loss: 3.7462\n",
            "   Batch 800/907, Loss: 3.8044\n",
            "   Batch 850/907, Loss: 3.3952\n",
            "   Batch 900/907, Loss: 3.3195\n",
            "Epoch 2/15 - Validating...\n",
            "S·ªë ph·∫ßn t·ª≠ trong batch: 3\n",
            "Ki·ªÉu d·ªØ li·ªáu batch: <class 'tuple'>\n",
            "  Ph·∫ßn t·ª≠ 0: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 1: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 2: type=<class 'torch.Tensor'>, shape=torch.Size([32])\n",
            "\n",
            "üìä Epoch 2 Summary:\n",
            "   Train Loss: 3.6181\n",
            "   Val Loss:   4.1289\n",
            "   Learning Rate: 0.001000\n",
            "   üíæ Best model saved! (Loss: 4.1289)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 3/15 - Training...\n",
            "   Batch 0/907, Loss: 3.3047\n",
            "   Batch 50/907, Loss: 2.8158\n",
            "   Batch 100/907, Loss: 2.6771\n",
            "   Batch 150/907, Loss: 3.1113\n",
            "   Batch 200/907, Loss: 3.4542\n",
            "   Batch 250/907, Loss: 3.8340\n",
            "   Batch 300/907, Loss: 3.1422\n",
            "   Batch 350/907, Loss: 3.0546\n",
            "   Batch 400/907, Loss: 3.1190\n",
            "   Batch 450/907, Loss: 3.3805\n",
            "   Batch 500/907, Loss: 2.9956\n",
            "   Batch 550/907, Loss: 2.7561\n",
            "   Batch 600/907, Loss: 2.9891\n",
            "   Batch 650/907, Loss: 2.9236\n",
            "   Batch 700/907, Loss: 3.2615\n",
            "   Batch 750/907, Loss: 3.3386\n",
            "   Batch 800/907, Loss: 2.9911\n",
            "   Batch 850/907, Loss: 2.9576\n",
            "   Batch 900/907, Loss: 3.0073\n",
            "Epoch 3/15 - Validating...\n",
            "S·ªë ph·∫ßn t·ª≠ trong batch: 3\n",
            "Ki·ªÉu d·ªØ li·ªáu batch: <class 'tuple'>\n",
            "  Ph·∫ßn t·ª≠ 0: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 1: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 2: type=<class 'torch.Tensor'>, shape=torch.Size([32])\n",
            "\n",
            "üìä Epoch 3 Summary:\n",
            "   Train Loss: 3.1240\n",
            "   Val Loss:   3.7494\n",
            "   Learning Rate: 0.001000\n",
            "   üíæ Best model saved! (Loss: 3.7494)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 4/15 - Training...\n",
            "   Batch 0/907, Loss: 2.4700\n",
            "   Batch 50/907, Loss: 2.6772\n",
            "   Batch 100/907, Loss: 2.7185\n",
            "   Batch 150/907, Loss: 2.7180\n",
            "   Batch 200/907, Loss: 2.8080\n",
            "   Batch 250/907, Loss: 2.3225\n",
            "   Batch 300/907, Loss: 2.9369\n",
            "   Batch 350/907, Loss: 2.3799\n",
            "   Batch 400/907, Loss: 2.9914\n",
            "   Batch 450/907, Loss: 2.4752\n",
            "   Batch 500/907, Loss: 3.1109\n",
            "   Batch 550/907, Loss: 2.6952\n",
            "   Batch 600/907, Loss: 2.3597\n",
            "   Batch 650/907, Loss: 2.3153\n",
            "   Batch 700/907, Loss: 2.5305\n",
            "   Batch 750/907, Loss: 3.0613\n",
            "   Batch 800/907, Loss: 2.9034\n",
            "   Batch 850/907, Loss: 2.5873\n",
            "   Batch 900/907, Loss: 2.8373\n",
            "Epoch 4/15 - Validating...\n",
            "S·ªë ph·∫ßn t·ª≠ trong batch: 3\n",
            "Ki·ªÉu d·ªØ li·ªáu batch: <class 'tuple'>\n",
            "  Ph·∫ßn t·ª≠ 0: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 1: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 2: type=<class 'torch.Tensor'>, shape=torch.Size([32])\n",
            "\n",
            "üìä Epoch 4 Summary:\n",
            "   Train Loss: 2.7206\n",
            "   Val Loss:   3.5758\n",
            "   Learning Rate: 0.001000\n",
            "   üíæ Best model saved! (Loss: 3.5758)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 5/15 - Training...\n",
            "   Batch 0/907, Loss: 2.0851\n",
            "   Batch 50/907, Loss: 2.9400\n",
            "   Batch 100/907, Loss: 2.1853\n",
            "   Batch 150/907, Loss: 2.7495\n",
            "   Batch 200/907, Loss: 3.3298\n",
            "   Batch 250/907, Loss: 2.4275\n",
            "   Batch 300/907, Loss: 2.0623\n",
            "   Batch 350/907, Loss: 2.1913\n",
            "   Batch 400/907, Loss: 2.6457\n",
            "   Batch 450/907, Loss: 2.2476\n",
            "   Batch 500/907, Loss: 2.8548\n",
            "   Batch 550/907, Loss: 2.2179\n",
            "   Batch 600/907, Loss: 2.4401\n",
            "   Batch 650/907, Loss: 1.9090\n",
            "   Batch 700/907, Loss: 2.0013\n",
            "   Batch 750/907, Loss: 2.3124\n",
            "   Batch 800/907, Loss: 2.4744\n",
            "   Batch 850/907, Loss: 2.3452\n",
            "   Batch 900/907, Loss: 2.6992\n",
            "Epoch 5/15 - Validating...\n",
            "S·ªë ph·∫ßn t·ª≠ trong batch: 3\n",
            "Ki·ªÉu d·ªØ li·ªáu batch: <class 'tuple'>\n",
            "  Ph·∫ßn t·ª≠ 0: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 1: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 2: type=<class 'torch.Tensor'>, shape=torch.Size([32])\n",
            "\n",
            "üìä Epoch 5 Summary:\n",
            "   Train Loss: 2.4188\n",
            "   Val Loss:   3.4526\n",
            "   Learning Rate: 0.001000\n",
            "   üíæ Best model saved! (Loss: 3.4526)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 6/15 - Training...\n",
            "   Batch 0/907, Loss: 2.4135\n",
            "   Batch 50/907, Loss: 2.0046\n",
            "   Batch 100/907, Loss: 2.3014\n",
            "   Batch 150/907, Loss: 2.4602\n",
            "   Batch 200/907, Loss: 2.2041\n",
            "   Batch 250/907, Loss: 2.7717\n",
            "   Batch 300/907, Loss: 2.0061\n",
            "   Batch 350/907, Loss: 2.0380\n",
            "   Batch 400/907, Loss: 2.1401\n",
            "   Batch 450/907, Loss: 1.8543\n",
            "   Batch 500/907, Loss: 2.1600\n",
            "   Batch 550/907, Loss: 2.3479\n",
            "   Batch 600/907, Loss: 2.1782\n",
            "   Batch 650/907, Loss: 2.0197\n",
            "   Batch 700/907, Loss: 2.3111\n",
            "   Batch 750/907, Loss: 2.2027\n",
            "   Batch 800/907, Loss: 1.8083\n",
            "   Batch 850/907, Loss: 2.2679\n",
            "   Batch 900/907, Loss: 1.9294\n",
            "Epoch 6/15 - Validating...\n",
            "S·ªë ph·∫ßn t·ª≠ trong batch: 3\n",
            "Ki·ªÉu d·ªØ li·ªáu batch: <class 'tuple'>\n",
            "  Ph·∫ßn t·ª≠ 0: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 1: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 2: type=<class 'torch.Tensor'>, shape=torch.Size([32])\n",
            "\n",
            "üìä Epoch 6 Summary:\n",
            "   Train Loss: 2.1864\n",
            "   Val Loss:   3.3450\n",
            "   Learning Rate: 0.001000\n",
            "   üíæ Best model saved! (Loss: 3.3450)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 7/15 - Training...\n",
            "   Batch 0/907, Loss: 2.2824\n",
            "   Batch 50/907, Loss: 1.9821\n",
            "   Batch 100/907, Loss: 2.2407\n",
            "   Batch 150/907, Loss: 1.9083\n",
            "   Batch 200/907, Loss: 1.9711\n",
            "   Batch 250/907, Loss: 2.4898\n",
            "   Batch 300/907, Loss: 2.1517\n",
            "   Batch 350/907, Loss: 1.9020\n",
            "   Batch 400/907, Loss: 2.0338\n",
            "   Batch 450/907, Loss: 1.9883\n",
            "   Batch 500/907, Loss: 2.2158\n",
            "   Batch 550/907, Loss: 1.9972\n",
            "   Batch 600/907, Loss: 2.4661\n",
            "   Batch 650/907, Loss: 1.9056\n",
            "   Batch 700/907, Loss: 1.7451\n",
            "   Batch 750/907, Loss: 2.1910\n",
            "   Batch 800/907, Loss: 2.1725\n",
            "   Batch 850/907, Loss: 1.7280\n",
            "   Batch 900/907, Loss: 2.3605\n",
            "Epoch 7/15 - Validating...\n",
            "S·ªë ph·∫ßn t·ª≠ trong batch: 3\n",
            "Ki·ªÉu d·ªØ li·ªáu batch: <class 'tuple'>\n",
            "  Ph·∫ßn t·ª≠ 0: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 1: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 2: type=<class 'torch.Tensor'>, shape=torch.Size([32])\n",
            "\n",
            "üìä Epoch 7 Summary:\n",
            "   Train Loss: 2.0002\n",
            "   Val Loss:   3.2397\n",
            "   Learning Rate: 0.001000\n",
            "   üíæ Best model saved! (Loss: 3.2397)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 8/15 - Training...\n",
            "   Batch 0/907, Loss: 1.5097\n",
            "   Batch 50/907, Loss: 1.6073\n",
            "   Batch 100/907, Loss: 2.1396\n",
            "   Batch 150/907, Loss: 2.2848\n",
            "   Batch 200/907, Loss: 1.7527\n",
            "   Batch 250/907, Loss: 1.8249\n",
            "   Batch 300/907, Loss: 2.0327\n",
            "   Batch 350/907, Loss: 2.1681\n",
            "   Batch 400/907, Loss: 1.8241\n",
            "   Batch 450/907, Loss: 1.9197\n",
            "   Batch 500/907, Loss: 2.2239\n",
            "   Batch 550/907, Loss: 1.9475\n",
            "   Batch 600/907, Loss: 1.6789\n",
            "   Batch 650/907, Loss: 1.6411\n",
            "   Batch 700/907, Loss: 1.6960\n",
            "   Batch 750/907, Loss: 1.6045\n",
            "   Batch 800/907, Loss: 1.8293\n",
            "   Batch 850/907, Loss: 1.6948\n",
            "   Batch 900/907, Loss: 1.8678\n",
            "Epoch 8/15 - Validating...\n",
            "S·ªë ph·∫ßn t·ª≠ trong batch: 3\n",
            "Ki·ªÉu d·ªØ li·ªáu batch: <class 'tuple'>\n",
            "  Ph·∫ßn t·ª≠ 0: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 1: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 2: type=<class 'torch.Tensor'>, shape=torch.Size([32])\n",
            "\n",
            "üìä Epoch 8 Summary:\n",
            "   Train Loss: 1.8376\n",
            "   Val Loss:   3.1706\n",
            "   Learning Rate: 0.001000\n",
            "   üíæ Best model saved! (Loss: 3.1706)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 9/15 - Training...\n",
            "   Batch 0/907, Loss: 1.4742\n",
            "   Batch 50/907, Loss: 1.7771\n",
            "   Batch 100/907, Loss: 1.6362\n",
            "   Batch 150/907, Loss: 1.5184\n",
            "   Batch 200/907, Loss: 1.6046\n",
            "   Batch 250/907, Loss: 1.8555\n",
            "   Batch 300/907, Loss: 1.6876\n",
            "   Batch 350/907, Loss: 2.3619\n",
            "   Batch 400/907, Loss: 1.5656\n",
            "   Batch 450/907, Loss: 1.4475\n",
            "   Batch 500/907, Loss: 1.8413\n",
            "   Batch 550/907, Loss: 1.6461\n",
            "   Batch 600/907, Loss: 1.6825\n",
            "   Batch 650/907, Loss: 1.7329\n",
            "   Batch 700/907, Loss: 1.9377\n",
            "   Batch 750/907, Loss: 1.8650\n",
            "   Batch 800/907, Loss: 1.8304\n",
            "   Batch 850/907, Loss: 1.4727\n",
            "   Batch 900/907, Loss: 1.4871\n",
            "Epoch 9/15 - Validating...\n",
            "S·ªë ph·∫ßn t·ª≠ trong batch: 3\n",
            "Ki·ªÉu d·ªØ li·ªáu batch: <class 'tuple'>\n",
            "  Ph·∫ßn t·ª≠ 0: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 1: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 2: type=<class 'torch.Tensor'>, shape=torch.Size([32])\n",
            "\n",
            "üìä Epoch 9 Summary:\n",
            "   Train Loss: 1.6911\n",
            "   Val Loss:   3.1995\n",
            "   Learning Rate: 0.001000\n",
            "   ‚è≥ No improvement (1/3)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 10/15 - Training...\n",
            "   Batch 0/907, Loss: 1.5160\n",
            "   Batch 50/907, Loss: 1.6228\n",
            "   Batch 100/907, Loss: 1.4950\n",
            "   Batch 150/907, Loss: 1.1312\n",
            "   Batch 200/907, Loss: 1.4193\n",
            "   Batch 250/907, Loss: 1.4327\n",
            "   Batch 300/907, Loss: 1.5541\n",
            "   Batch 350/907, Loss: 1.5275\n",
            "   Batch 400/907, Loss: 1.6880\n",
            "   Batch 450/907, Loss: 1.3316\n",
            "   Batch 500/907, Loss: 1.3713\n",
            "   Batch 550/907, Loss: 1.2574\n",
            "   Batch 600/907, Loss: 1.3696\n",
            "   Batch 650/907, Loss: 1.1951\n",
            "   Batch 700/907, Loss: 1.6415\n",
            "   Batch 750/907, Loss: 1.5075\n",
            "   Batch 800/907, Loss: 1.7285\n",
            "   Batch 850/907, Loss: 2.0409\n",
            "   Batch 900/907, Loss: 1.4501\n",
            "Epoch 10/15 - Validating...\n",
            "S·ªë ph·∫ßn t·ª≠ trong batch: 3\n",
            "Ki·ªÉu d·ªØ li·ªáu batch: <class 'tuple'>\n",
            "  Ph·∫ßn t·ª≠ 0: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 1: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 2: type=<class 'torch.Tensor'>, shape=torch.Size([32])\n",
            "\n",
            "üìä Epoch 10 Summary:\n",
            "   Train Loss: 1.5674\n",
            "   Val Loss:   3.2334\n",
            "   Learning Rate: 0.001000\n",
            "   ‚è≥ No improvement (2/3)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 11/15 - Training...\n",
            "   Batch 0/907, Loss: 1.0947\n",
            "   Batch 50/907, Loss: 1.1291\n",
            "   Batch 100/907, Loss: 1.4053\n",
            "   Batch 150/907, Loss: 1.3780\n",
            "   Batch 200/907, Loss: 1.1522\n",
            "   Batch 250/907, Loss: 1.2341\n",
            "   Batch 300/907, Loss: 1.6956\n",
            "   Batch 350/907, Loss: 1.4305\n",
            "   Batch 400/907, Loss: 1.2820\n",
            "   Batch 450/907, Loss: 1.2636\n",
            "   Batch 500/907, Loss: 1.4661\n",
            "   Batch 550/907, Loss: 1.0537\n",
            "   Batch 600/907, Loss: 1.4615\n",
            "   Batch 650/907, Loss: 1.5785\n",
            "   Batch 700/907, Loss: 1.9087\n",
            "   Batch 750/907, Loss: 1.3793\n",
            "   Batch 800/907, Loss: 1.3828\n",
            "   Batch 850/907, Loss: 1.4992\n",
            "   Batch 900/907, Loss: 1.3386\n",
            "Epoch 11/15 - Validating...\n",
            "S·ªë ph·∫ßn t·ª≠ trong batch: 3\n",
            "Ki·ªÉu d·ªØ li·ªáu batch: <class 'tuple'>\n",
            "  Ph·∫ßn t·ª≠ 0: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 1: type=<class 'torch.Tensor'>, shape=torch.Size([32, 27])\n",
            "  Ph·∫ßn t·ª≠ 2: type=<class 'torch.Tensor'>, shape=torch.Size([32])\n",
            "\n",
            "üìä Epoch 11 Summary:\n",
            "   Train Loss: 1.4659\n",
            "   Val Loss:   3.1798\n",
            "   Learning Rate: 0.001000\n",
            "   ‚è≥ No improvement (3/3)\n",
            "\n",
            "üõë Early stopping triggered after 11 epochs!\n",
            "   Best validation loss: 3.1706\n",
            "\n",
            "üéâ Training completed!\n",
            "   Best validation loss: 3.1706\n",
            "   Total epochs trained: 11\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 15 # Train 10 v√≤ng\n",
        "CLIP = 1\n",
        "\n",
        "train_losses, val_losses = train(model, train_iterator, valid_iterator,\n",
        "                                 criterion, optimizer,\n",
        "                                 N_EPOCHS, device, clip=CLIP,\n",
        "                                 teacher_forcing_ratio=0.5,\n",
        "                                 patience=3, model_path='best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2y2v9V2JOrxF",
      "metadata": {
        "id": "2y2v9V2JOrxF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "bb2cd3aa-9418-4897-d205-c1fb4ed920c5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Bi·ªÉu ƒë·ªì hu·∫•n luy·ªán Model LSTM')\n",
        "plt.savefig('loss_chart.png') # L∆∞u ·∫£nh\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "_5at0VL9Osow",
      "metadata": {
        "id": "_5at0VL9Osow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce5d138-f6e0-478d-ccd7-917a6bf6f586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ƒêang t·∫£i l·∫°i t·∫≠p Test ƒë·ªÉ l·∫•y v√≠ d·ª•...\n",
            "--> ƒê√£ t·∫£i xong test_dataset v·ªõi 1000 c√¢u.\n",
            "Src: A mother and her young song enjoying a beautiful day outside.\n",
            "Trg: Une m√®re et son jeune fils profitant d'une belle journ√©e dehors.\n",
            "Pred: une m√®re m√®re et sa fille profitent d' une f√™te journ√©e journ√©e journ√©e .\n"
          ]
        }
      ],
      "source": [
        "# --- ƒêO·∫†N CODE V√Å L·ªñI ---\n",
        "from src.dataset import TranslationDataset, build_vocab_and_tokenizers\n",
        "\n",
        "# 1. L·∫•y l·∫°i b·ªô t·ª´ ƒëi·ªÉn v√† tokenizer\n",
        "# src_vocab, trg_vocab = build_vocab_and_tokenizers()\n",
        "\n",
        "# 2. Kh·ªüi t·∫°o l·∫°i bi·∫øn test_dataset th·ªß c√¥ng\n",
        "print(\"ƒêang t·∫£i l·∫°i t·∫≠p Test ƒë·ªÉ l·∫•y v√≠ d·ª•...\")\n",
        "test_dataset = TranslationDataset(\n",
        "    'data/raw/test_2016_flickr.en',\n",
        "    'data/raw/test_2016_flickr.fr',\n",
        "    src_vocab, trg_vocab, src_tokenizer, trg_tokenizer\n",
        ")\n",
        "\n",
        "print(f\"--> ƒê√£ t·∫£i xong test_dataset v·ªõi {len(test_dataset)} c√¢u.\")\n",
        "# Load model t·ªët nh·∫•t\n",
        "# model.load_state_dict(torch.load('best_model.pth'))\n",
        "checkpoint = torch.load('best_model.pth', map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "\n",
        "# H√†m d·ªãch (C·∫ßn ch·ªânh s·ª≠a t√πy v√†o c√°ch b·∫°n tokenize)\n",
        "def translate_sentence(sentence, src_tokenizer, model, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Tokenize\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = src_tokenizer(sentence)\n",
        "    else:\n",
        "        tokens = sentence\n",
        "\n",
        "    # 2. Convert to index\n",
        "    src_indexes = [src_vocab['<sos>']] + [src_vocab[t] for t in tokens] + [src_vocab['<eos>']]\n",
        "\n",
        "    # 3. Tensor h√≥a\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)  # [1, seq_len]\n",
        "    src_len = torch.tensor([len(src_indexes)], dtype=torch.long).to(device)\n",
        "\n",
        "    # 4. Encode\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden, encoder_cell = model.encoder(src_tensor, src_len)\n",
        "\n",
        "    # decoder hidden/cell init\n",
        "    hidden = encoder_hidden\n",
        "    cell = encoder_cell\n",
        "\n",
        "    # 5. B·∫Øt ƒë·∫ßu decode\n",
        "    trg_indexes = [trg_vocab['<sos>']]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).unsqueeze(0).to(device)  # [1,1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(\n",
        "                trg_tensor,      # [1,1]\n",
        "                hidden,          # hidden state\n",
        "                cell,            # cell state\n",
        "                encoder_hidden   # context c·ªë ƒë·ªãnh t·ª´ encoder\n",
        "            )\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "    trg_tokens = [trg_vocab.lookup_token(i) for i in trg_indexes]\n",
        "\n",
        "    return trg_tokens[1:-1]   # b·ªè <sos> v√† <eos>\n",
        "\n",
        "\n",
        "# Th·ª≠ d·ªãch 1 c√¢u trong t·∫≠p test\n",
        "# --- S·ª¨A L·∫†I ƒêO·∫†N L·∫§Y V√ç D·ª§ ---\n",
        "\n",
        "# 1. Ch·ªçn index\n",
        "example_idx = 10\n",
        "\n",
        "# 2. L·∫•y tr·ª±c ti·∫øp t·ª´ list ch·ª©a text th√¥ (Raw text)\n",
        "# L∆∞u √Ω: Class TranslationDataset m√¨nh ƒë∆∞a b·∫°n l∆∞u text trong bi·∫øn .src_data\n",
        "src = test_dataset.src_data[example_idx].strip()\n",
        "trg = test_dataset.trg_data[example_idx].strip()\n",
        "\n",
        "print(f'Src: {src}')\n",
        "print(f'Trg: {trg}')\n",
        "\n",
        "# 3. D·ªãch\n",
        "# H√†m translate_sentence nh·∫≠n v√†o chu·ªói string, n√™n ƒë∆∞a th·∫≥ng src v√†o\n",
        "translation = translate_sentence(src, src_tokenizer, model, device)\n",
        "\n",
        "print(f'Pred: {\" \".join(translation)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "MBSaCTD9OwR6",
      "metadata": {
        "id": "MBSaCTD9OwR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02bc0427-cc75-4386-a9ad-12ea242fd1ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ƒêang t√≠nh to√°n BLEU tr√™n 1000 c√¢u...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:09<00:00, 101.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU score = 22.78\n"
          ]
        }
      ],
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "from tqdm import tqdm  # Th∆∞ vi·ªán t·∫°o thanh ti·∫øn tr√¨nh (c·∫ßn c√†i: pip install tqdm)\n",
        "\n",
        "def calculate_bleu(data, model, device, max_len=50):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "\n",
        "    # Chuy·ªÉn model sang ch·∫ø ƒë·ªô ƒë√°nh gi√°\n",
        "    model.eval()\n",
        "\n",
        "    # D√πng tqdm b·ªçc range l·∫°i ƒë·ªÉ hi·ªÉn th·ªã thanh loading\n",
        "    print(f\"ƒêang t√≠nh to√°n BLEU tr√™n {len(data)} c√¢u...\")\n",
        "    for i in tqdm(range(len(data)), desc=\"Translating\"):\n",
        "\n",
        "        # 1. L·∫•y c√¢u g·ªëc v√† c√¢u ƒë√≠ch d·∫°ng chu·ªói\n",
        "        src_raw = data.src_data[i].strip()\n",
        "        trg_raw = data.trg_data[i].strip()\n",
        "\n",
        "        # 2. Model d·ªãch v√† tr·∫£ v·ªÅ list token\n",
        "        # L∆∞u √Ω: H√†m translate_sentence n√†y l·∫•y t·ª´ ƒëo·∫°n code tr∆∞·ªõc\n",
        "        pred_trg = translate_sentence(src_raw, src_tokenizer, model, device, max_len)\n",
        "\n",
        "        # 3. Tokenize c√¢u ƒë√≠ch (Ground Truth)\n",
        "        # S·ª¨A L·ªñI: D√πng trg_tokenizer thay v√¨ g·ªçi h√†m tokenize_fr l·∫°\n",
        "        if isinstance(trg_raw, str):\n",
        "            trg_tokenized = trg_tokenizer(trg_raw)\n",
        "        else:\n",
        "            trg_tokenized = list(trg_raw)\n",
        "\n",
        "        # --- L√†m s·∫°ch (Filter) ---\n",
        "        # Lo·∫°i b·ªè c√°c token kh√¥ng c·∫ßn thi·∫øt ƒë·ªÉ t√≠nh ƒëi·ªÉm ch√≠nh x√°c h∆°n\n",
        "        ignore_tokens = [\"<sos>\", \"<eos>\", \"<pad>\", \"<unk>\"]\n",
        "\n",
        "        pred_trg_clean = [tok for tok in pred_trg if tok not in ignore_tokens]\n",
        "        trg_tokenized_clean = [tok for tok in trg_tokenized if tok not in ignore_tokens]\n",
        "\n",
        "        # 4. Th√™m v√†o list theo ƒë√∫ng format torchtext y√™u c·∫ßu\n",
        "        pred_trgs.append(pred_trg_clean)\n",
        "        trgs.append([trg_tokenized_clean])    # Reference ph·∫£i b·ªçc trong 1 list n·ªØa (v√¨ 1 c√¢u src c√≥ th·ªÉ c√≥ nhi·ªÅu c√¢u trg)\n",
        "\n",
        "    # --- T√≠nh BLEU ---\n",
        "    return bleu_score(pred_trgs, trgs)\n",
        "\n",
        "# --- CH·∫†Y TH·ª¨ ---\n",
        "# L∆∞u √Ω: Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t 1-5 ph√∫t t√πy ƒë·ªô l·ªõn t·∫≠p test v√† GPU\n",
        "score = calculate_bleu(test_dataset, model, device)\n",
        "print(f\"\\nBLEU score = {score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5xzV1z5oWkw",
        "outputId": "941ebe12-64a1-4f34-a05a-7c41a64e7c79"
      },
      "id": "N5xzV1z5oWkw",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mmodified:   best_model.pth\u001b[m\n",
            "\t\u001b[31mmodified:   loss_chart.png\u001b[m\n",
            "\t\u001b[31mmodified:   src/__pycache__/__init__.cpython-312.pyc\u001b[m\n",
            "\t\u001b[31mmodified:   src/__pycache__/dataset.cpython-312.pyc\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31msrc/__pycache__/model_base.cpython-312.pyc\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}